{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194a1b54",
   "metadata": {},
   "source": [
    "# idea\n",
    "### background\n",
    "Large Language Models have been growing in popularity, especially as chatbots and psuedo-search engines. But, how could they be used to benefit our work?\n",
    "\n",
    "If you load up an LLM from a source like OpenAI, you're getting access to a model that has already been trained on text data and is familiar with navigating at least English text.\n",
    "- we have tested using English prompts on Spanish documents, and it appears plausible to use this process and get similar quality in results without use of a translator API. Because the model can be instructed to return snippets of a document in its response ([among other things](https://platform.openai.com/docs/guides/prompt-engineering)), you have the option to capture the raw, Spanish text and the model's working English translation of it with the rest of the output for transparency and auditing. \n",
    "- I don't believe we've tested using strictly non-English prompts & responses, but it looks like [someone else has](https://community.openai.com/t/force-api-response-to-be-in-non-english-language-how/175381/2). If you go that route, let us know how it goes!\n",
    "\n",
    "Using an LLM through an API, we can feed in a document and then prompt the model with specific questions to \"extract\" information from the document. In this way, we avoid asking the model to use referential \"knowledge\" (info gleaned from its training data, which we don't know about or trust to be accurate) in favor of relying on what it found in the input document. \n",
    "\n",
    "- quick disclaimer: I'm going to talk about OpenAI as the core tool here, but we're actually going to use LangChain in our code because it provides some more user-friendly wrappings to the same underlying functionality.\n",
    "\n",
    "### use case\n",
    "In the DPA report data, we have text data in the form of `findings_of_fact`, which can be anywhere from a sentence or two to multiple pages worth of text. To better prepare the reports for review by the busy Public Defender's Office, we want to test an LLM's ability to identify and summarize the meaningful justifications provided in the text for the `finding` given.\n",
    "\n",
    "For example, in the document:\n",
    "> \"The complainants described the officers as having been angry and peaking rudely to them, but did not corroborate one another in any detailed manner. The officers denied that any of them were rude or angry. There were no independent witnesses to the interaction between officers and complainants. There was insufficient evidence to prove or disprove the allegation.\"\n",
    "\n",
    "If we ask the model to list the justifications, we should get back all of these snippets:\n",
    "- \"Complainants did not corroborate one another in any detailed manner\"\n",
    "- \"The officers denied that any of them were rude or angry.\"\n",
    "- \"There were no independent witnesses to the interaction between officers and complainants.\"\n",
    "\n",
    "We store some other expected and verified responses in a hand file, `\"../hand/examples.yml\"`.\n",
    "\n",
    "# security + privacy\n",
    "\n",
    "### credentials\n",
    "##### A VERY IMPORTANT note\n",
    "In order to access one of these models, you will need to have credentials in the form of an **API key**. Similarly to SSH keys, part of this is private and it is extremely important not to share or (accidentally) make it public. Without the secret part, you won't be able to do the thing. \n",
    "\n",
    "When you sign up for an account with OpenAI, you will have an [\"API Keys\" tab](https://platform.openai.com/api-keys) where you can make new keys and view existing ones. ***Crucially, when you generate a new key, the secret part will be shown to you once during that process, and then never again.*** If you lose it or make it public, you will need to make a new one. That being said, trashing and re-generating keys is an easy and effective action for if you're worried that your key may have been leaked, so use that option freely.\n",
    "\n",
    "In this demo, and in other projects, I store my private key in a text file in a credentials folder of my dotfiles, `\"../../../dotfiles/creds/openai\"`. Since my dotfiles get pushed to GitHub, I also have a `.gitignore` file in that repo that ignores the `creds` folder, so that I don't accidentally make it public. With this structure, I can use a handy `getcreds()` method that just reads the text file and loads the key into this notebook environment, without me pasting it here or into a `.bashrc` file.\n",
    "- This notebook is setup for this style of credentialling because it's what I use, but there are other ways to do this kind of thing secretly and flexibly, each one has their tradeoffs. Please feel free to explore other methods or use what you know already!\n",
    "\n",
    "### document sharing\n",
    "In this demo, we use publicly available report data and supply the model with text snippets from those documents. However, if you wanted to use data that may contain PII or other sensitive information, I highly recommend reviewing [OpenAI's FAQs](https://help.openai.com/en/articles/7039943-data-usage-for-consumer-services-faq) and current privacy policy and deciding for yourself if it's an appropriate tool to use. At the moment of this demo's creation, mid-Dec 2023, it's our understanding that the documents basically only live on OpenAI's servers for the duration of the API call, and are otherwise not stored or kept by the company. However, privacy policies can be changed at any time and you don't agree to any permissions when you sign-up, so it's unclear what we should expect in perpetuity.\n",
    "\n",
    "In lieu of sharing entire plain-text documents, you can also consider limiting the shared text to a specific section that would leave the model context-unaware or not include the sensitive info. Alternatively, vectorizing with an embedding tool could provide an optimized run and better privacy. \n",
    "\n",
    "# OpenAI setup\n",
    "Python used here, but other languages are available.\n",
    "- [ ] sign up for an account at [openai.com](https://openai.com/)\n",
    "- [ ] if applicable, follow a link to join an organization already registered on the platform\n",
    "    - **note about using an 'organization' during signup:** the platform doesn't intuitively merge identically named organizations you're connected to, so if you list \"HRDAG\" when you sign up and then join \"hrdag\" via invite later, you will be a member of two organizations with separate IDs. If you then rename \"HRDAG\" to \"hrdag\", it won't prompt you to pick a different name. You will just be a member of two identically-named \"hrdag\" teams, one with other team members and account funds, and one with just you that was generated and renamed by you.\n",
    "    - The Organizations tab in Settings has an editable name field and an uneditable \"Organization ID\" field. There's no option to delete the org.\n",
    "    - The Team tab in Settings has an option to Leave, but you can't leave a team you are the sole Owner of without transferring ownership, even if you are also the only member.\n",
    "    - No help topics about deleting the org, closest is deleting account. I don't feel like contacting support about this. I renamed the org I signed up under as \"hrdag-backup\". \n",
    "- [ ] follow the first few steps in the [developer quickstart guide](https://platform.openai.com/docs/quickstart?context=python)\n",
    "    - [ ] install the `openai`, `langchain` python packages\n",
    "    - [ ] setup the `OPENAI_API_KEY` either as an environment variable (described in the quickstart guide) or a creds file (as described in \"Credentials\" above)\n",
    "    - [ ] try the `openai-test.py` script under \"Sending your first API request\" with the suggested code and make sure it runs successfully\n",
    "\n",
    "\n",
    "# references\n",
    "- Ayyub Ibrahim's [llm-criminal-justice-research](https://github.com/ayyubibrahimi/llm-criminal-justice-research/tree/main)\n",
    "- Tarak and Ayyub's blogpost, [Using large language models for structured information extraction from the Innocence Project New Orleans' wrongful conviction case files](https://hrdag.org/tech-notes/large-language-models-IPNO.html)\n",
    "- Tristan Chambers' 30 Nov BIDS seminar, [Unpacking police violence and misconduct records: Solving information extraction challenges using Large Language Models (LLMs)](https://events.berkeley.edu/BIDS/event/209002-bids-seminar-with-tristan-chambers)\n",
    "- [Answering Question About Custom Documents Using LangChain (and OpenAI)](https://kleiber.me/blog/2023/02/25/question-answering-using-langchain/)\n",
    "\n",
    "Also:\n",
    "- LangChain's [Introduction docs](https://python.langchain.com/docs/get_started/introduction)\n",
    "- OpenAI's [Prompt Engineering guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "\n",
    "# jupyter note\n",
    "When I want to demo or explore something, I rely on the [`runtools`](https://github.com/ipython-contrib/jupyter_contrib_nbextensions/tree/master/src/jupyter_contrib_nbextensions/nbextensions/runtools) and [`codefolding`](https://github.com/ipython-contrib/jupyter_contrib_nbextensions/tree/master/src/jupyter_contrib_nbextensions/nbextensions/codefolding) Jupyter notebook extensions to mark and collapse key setup cells, so that exploratory and analytical work stays in focus. This is totally optional, but I wanted to mention it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f306788",
   "metadata": {},
   "source": [
    "# setup for the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f9d1dd7",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# depedencies\n",
    "from random import randint\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e67613b",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# support methods\n",
    "def getcreds():\n",
    "    with open('../../../../dotfiles/creds/openai') as f:\n",
    "        out = f.readline().strip()\n",
    "    return out\n",
    "\n",
    "\n",
    "def read_yaml(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_template(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        out = f.readlines()\n",
    "    return ''.join(out)\n",
    "\n",
    "\n",
    "def getex():\n",
    "    return examples[randint(1,len(examples))]\n",
    "\n",
    "\n",
    "def getdoc():\n",
    "    return allegs.findings_of_fact.sample(1).values[0]\n",
    "\n",
    "\n",
    "def query_model(doc, ex=None):\n",
    "    if not ex: ex = getex()\n",
    "    res = chain.invoke({'EX_DOCUMENT': ex['DOCUMENT'],\n",
    "                        'EX_RESPONSE': ex['RESPONSE'],\n",
    "                        'DOCUMENT': doc})\n",
    "    return res.content.strip()\n",
    "\n",
    "\n",
    "def getreport(samp):\n",
    "    rep = f\"\"\"\n",
    "    ALLEGATION(S):\\t{samp.allegations.values}\n",
    "    FINDING:\\t{samp.finding.values}\n",
    "\n",
    "\n",
    "    QUERY RESPONSE:\\n{samp.q_justifications.values}\n",
    "\n",
    "    ---\n",
    "    DOCUMENT (AKA, \"FINDINGS OF FACT\"):\\n\n",
    "    {samp.findings_of_fact.values}\n",
    "    ---\n",
    "    \"\"\"\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8726c3a5",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# main\n",
    "# setup\n",
    "OPENAI_API_KEY = getcreds()\n",
    "examples = read_yaml(\"../hand/examples.yml\")\n",
    "templ = read_template(\"../hand/template.txt\")\n",
    "\n",
    "# get the data\n",
    "allegs = pd.read_parquet(\"../input/allegations.parquet\", columns=[\n",
    "    'allegation_id', 'allegations', 'finding', 'findings_of_fact']).dropna()\n",
    "\n",
    "# this is about context window length\n",
    "# which is worth reading about but isn't discussed here\n",
    "allegs['ntokens'] = allegs.findings_of_fact.apply(lambda x: len(x.split()))\n",
    "allegs = allegs.loc[allegs.ntokens < 4000].copy()\n",
    "\n",
    "# setup the model\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0, api_key=OPENAI_API_KEY)\n",
    "prompt = ChatPromptTemplate.from_template(templ)\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0dc66",
   "metadata": {},
   "source": [
    "### preview allegation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52d91d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>15082</th>\n",
       "      <th>10721</th>\n",
       "      <th>21654</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allegation_id</th>\n",
       "      <td>a7eb190ad309f884</td>\n",
       "      <td>ece472d40320f4b0</td>\n",
       "      <td>190c72b7e4f0de2c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allegations</th>\n",
       "      <td>1: The officer entered the residence.</td>\n",
       "      <td>1: The officer behaved and spoke inappropriate...</td>\n",
       "      <td>1: The officer arrested the complainant withou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finding</th>\n",
       "      <td>PC</td>\n",
       "      <td>NS</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>findings_of_fact</th>\n",
       "      <td>The San Francisco Police Department officers o...</td>\n",
       "      <td>The complainant stated the named officer mistr...</td>\n",
       "      <td>The officer conducted a warrants check through...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ntokens</th>\n",
       "      <td>81</td>\n",
       "      <td>47</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              15082  \\\n",
       "allegation_id                                      a7eb190ad309f884   \n",
       "allegations                  1: The officer entered the residence.    \n",
       "finding                                                          PC   \n",
       "findings_of_fact  The San Francisco Police Department officers o...   \n",
       "ntokens                                                          81   \n",
       "\n",
       "                                                              10721  \\\n",
       "allegation_id                                      ece472d40320f4b0   \n",
       "allegations       1: The officer behaved and spoke inappropriate...   \n",
       "finding                                                          NS   \n",
       "findings_of_fact  The complainant stated the named officer mistr...   \n",
       "ntokens                                                          47   \n",
       "\n",
       "                                                              21654  \n",
       "allegation_id                                      190c72b7e4f0de2c  \n",
       "allegations       1: The officer arrested the complainant withou...  \n",
       "finding                                                          PC  \n",
       "findings_of_fact  The officer conducted a warrants check through...  \n",
       "ntokens                                                         108  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allegs.sample(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890df7b",
   "metadata": {},
   "source": [
    "# prompt + examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc8939",
   "metadata": {},
   "source": [
    "### about the template\n",
    "The prompt you supply the LLM with has a direct effect on the response you get, including, for example:\n",
    "- **format:** whether the response is a complete sentence or a piece of data\n",
    "- **ambiguity handling:** whether the model supplies a response like \"No\" when \"Unclear\" or \"Not mentioned\" might have been more accurate\n",
    "- **citations:** whether the model includes a snippet of the text in its response and/or the location of the text in the larger document\n",
    "\n",
    "You also have the option to supply things like background information and a role/persona to the model through the prompt. \n",
    "\n",
    "Recommended reading if you get into using these tools:\n",
    "- [Prompt Engineering](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [LangChain Templates](https://python.langchain.com/docs/templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496a2d72",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Backstory\n",
      "In the last 30 years, allegations of police misconduct have been investigated by one of two agencies, depending on when the allegation was received and opened for investigation:\n",
      "- the Office of Citizen Complaints, or the \"OCC\" for short.\n",
      "- the Department of Police Accountability, or the \"DPA\" for short.\n",
      "Each month, the agency publishes a report that includes their findings related to each investigation.\n",
      "\n",
      "# Assignment\n",
      "Your role: You are an AI assistant retrieving information from the reports published by these agencies for your team to review.\n",
      "Your focus: The reasons for the conclusion.\n",
      "\n",
      "# Example\n",
      "Here is an example document and the correct response for that document:\n",
      "Ex)\n",
      "{EX_DOCUMENT}\n",
      "\n",
      "Response:\n",
      "{EX_RESPONSE}\n",
      "\n",
      "# Query\n",
      "Now, below is the document for you to review. \n",
      "What are the reasons given by the investigating agency for their finding?\n",
      "- When you identify a reason in the text, add it to a list and then just give me that list.\n",
      "- If the allegation was mediated, say that the allegation was mediated and those findings are not public.\n",
      "- If you don't find any `reason`s in the `text`, say that no reasons were identified.\n",
      "- Always include the last sentence of the document in your response.\n",
      "\n",
      "---\n",
      "\n",
      "{DOCUMENT}\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(templ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "201a0e5b",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# get an example set\n",
    "ex = getex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b7a59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The complainant stated that her son was stopped for no apparent reason. She was not present during the traffic stop. The named officers stated they were driving behind the complainant’s son’s vehicle when they observed that the vehicle had expired registration tabs. Records from the California Department of Motor Vehicles indicate that the vehicle in question had expired registration when the vehicle was stopped. Department General Order 5.03 allows a police officer to briefly detain a person for questioning or request identification only if the officer has reasonable suspicion that the person’s behavior is related to criminal activity. The evidence proved that the act, which provided the basis for the allegation, occurred. However, the act was justified, lawful and proper.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex['DOCUMENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "557e19d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PC'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex['FINDING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe289696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The named officers stated they observed that the vehicle had expired registration tabs.',\n",
       " 'Records from the California Department of Motor Vehicles indicate that the vehicle in question had expired registration when the vehicle was stopped.',\n",
       " 'Department General Order 5.03 allows a police officer to briefly detain a person for questioning or request identification only if the officer has reasonable suspicion that the person’s behavior is related to criminal activity.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex['RESPONSE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c7f8c",
   "metadata": {},
   "source": [
    "# explore 1: single query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11a15966",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# get a single doc\n",
    "doc = getdoc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a3569d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Department records showed that the complainant was detained during a homicide investigation.', 'Records also showed that the inspector assigned to the homicide investigation has retired from the Department.', 'The identity of the alleged officer could not be established.']\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_model(doc=doc, ex=ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1183ee",
   "metadata": {},
   "source": [
    "#### is the above a reasonable response for the below document?\n",
    "Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46ada705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In her written complaint, the complainant stated that she was strip-searched without cause. The complainant did not provide an interview. Department records showed that the complainant was detained during a homicide investigation, and that the SFPD’s Tactical Unit was at the scene. Records also showed that the inspector assigned to the homicide investigation has retired from the Department. In addition, the officer in charge of the Tactical Unit has also retired from the Department. The identity of the alleged officer could not be established. DATE OF COMPLAINT: 12/04/13                DATE OF COMPLETION: 07/31/17                PAGE# 2 of 2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a227d7",
   "metadata": {},
   "source": [
    "Is there any detail missing that should be included, or vice versa? Should this be a reason to fine-tune our template/prompt? Is it understandable, fair, or acceptable for the model to respond this way given this kind of info?\n",
    "\n",
    "Are there any other questions to stop and ask before we deploy this method on a larger sample?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51598c2",
   "metadata": {},
   "source": [
    "# explore 2: batch querying\n",
    "This approach is still experimental to us, but we're very curious about how this might work on a larger document collection. Let's take a sample of the DPA allegation data and run `query_model()` over the `findings_of_fact` \"document\"s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bae5fee7",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "test_allegs = allegs[['allegation_id', 'allegations', 'finding', 'findings_of_fact']].sample(20).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e986259",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "test_allegs['q_justifications'] = test_allegs.findings_of_fact.apply(query_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ef5ceb",
   "metadata": {},
   "source": [
    "### review justification(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96e36ad0",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "samp = test_allegs.sample(1)\n",
    "rep = getreport(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a895cd8",
   "metadata": {},
   "source": [
    "#### in the below report, is the LLM's reponse valid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2ce1cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ALLEGATION(S):\t['1: The officer failed to take required action. ']\n",
      "    FINDING:\t['PC']\n",
      "\n",
      "\n",
      "    QUERY RESPONSE:\n",
      "[\"['The officer correctly determined she did not have sufficient evidence to make an arrest.', 'The officer explained the citizens arrest process to the complainant.', 'The officer properly investigated and documented the incident in a report.']\"]\n",
      "\n",
      "    ---\n",
      "    DOCUMENT (AKA, \"FINDINGS OF FACT\"):\n",
      "\n",
      "    ['The complainant stated the officer negligently responded to his call for service. The complainant alleged his neighbor assaulted him and the officer failed to arrest the neighbor. The officer responded, along with two backup officers. The officer interviewed the complainant and the neighbor. The officer also interviewed the complainant’s landlord. There were no witnesses to the alleged assault and the complainant was not injured. The officer correctly determined she did not have sufficient evidence to make an arrest. The officer explained the citizens arrest process to the complainant. The officer properly investigated and documented the incident in a report. The evidence proved that the act which provided the basis for the allegations occurred; however such acts were justified, lawful and proper.']\n",
      "    ---\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca9940",
   "metadata": {},
   "source": [
    "# explore 3:  'SUSTAINED' allegation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15c1a2c2",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "sust = allegs.loc[allegs.finding == 'S'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5b7310d",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "sust_samp = sust.sample()\n",
    "sust_samp['q_justifications'] = sust_samp.findings_of_fact.apply(query_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e99585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ALLEGATION(S):\t['11-12: The officers detained the complainant without justification. ']\n",
      "    FINDING:\t['S']\n",
      "\n",
      "\n",
      "    QUERY RESPONSE:\n",
      "[\"['Entering a residence without a search warrant, consent or exigent circumstance is prohibited by Department General Orders, California State law and the Fourth Amendment to the United States Constitution.', 'The allegation was sustained.']\"]\n",
      "\n",
      "    ---\n",
      "    DOCUMENT (AKA, \"FINDINGS OF FACT\"):\n",
      "\n",
      "    ['The officers stated they detained the complainant when the complainant refused to allow officers to enter the residence without a search warrant. The officers acknowledged that they did not have a search warrant. Entering a residence without a search warrant, consent or exigent circumstance is prohibited by Department General Orders, California State law and the Fourth Amendment to the United States Constitution. The allegation was sustained.']\n",
      "    ---\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(getreport(sust_samp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f832ba",
   "metadata": {},
   "source": [
    "- Is the model missing or under-reporting justifications?\n",
    "- Is it over-reporting sentences that are not related to the justification?\n",
    "\n",
    "These findings are constructed with an audience of lawyers in mind, and the boundaries of legal justifications compared to other statements might be a problem for this kind of prompting. Our PDO partners might be able to intuitively audit the responses and know when something is off about an individual case, but this experiment seems to have been a better exploration than application test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07dda11",
   "metadata": {},
   "source": [
    "# explore 4: precision + quality\n",
    "This is an interesting template that might benefit from fine-tuning, but let's try a more structured question that we can compare to ground truth data to get a better idea of how the model is actually doing. \n",
    "\n",
    "Let's go back to our dataset and pull the `complaint_meta` column, which contains the raw text from the top of the document where the `date_complained` and `date_completed` fields live. We can also use the `time_to_complete` variable as an extra challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fba9bd9",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# re-read allegation data, with a focus on date fields\n",
    "dates = pd.read_parquet(\"../input/allegations.parquet\", columns=[\n",
    "    'allegation_id', 'complaint_meta',\n",
    "    'date_complained', 'date_completed', 'time_to_complete']).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be2906",
   "metadata": {},
   "source": [
    "Then we can reframe the prompt around extracting the date info that we already have, and see how accurate the responses actually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79a4dcbe",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# reset setup, with a focus on date fields\n",
    "def query_dates(doc):\n",
    "    res = date_chain.invoke({'DOCUMENT': doc})\n",
    "    return res.content.strip()\n",
    "\n",
    "\n",
    "date_tmpl = \"\"\"\n",
    "# Backstory\n",
    "In the last 30 years, allegations of police misconduct have been investigated by one of two agencies, depending on when the allegation was received and opened for investigation:\n",
    "- the Office of Citizen Complaints, or the \"OCC\" for short.\n",
    "- the Department of Police Accountability, or the \"DPA\" for short.\n",
    "Each month, the agency publishes a report that includes their findings related to each investigation.\n",
    "\n",
    "# Assignment\n",
    "Your role: You are an AI assistant retrieving information from the reports published by these agencies for your team to review.\n",
    "Your focus: Collecting the date of the complaint and when the investigation into it was closed.\n",
    "\n",
    "# Query\n",
    "Now, below is the document for you to review.\n",
    "Please respond with ONLY these 3 pieces of information in a numbered list:\n",
    "    1. The date the complaint was made, as just \"%Y-%m-%d\". Do not write a complete sentence.\n",
    "    2. The date the investigation was completed, as just \"%Y-%m-%d\". Do not write a complete sentence.\n",
    "    3. How much time passed between the completed date and the complained date, as \"n days\"?\n",
    "\n",
    "Also,\n",
    "- If the date is incomplete, cannot be parsed or formatted, say that.\n",
    "- If you could not find and format both dates, or there was a different problem calculating how many days passed, return 'None'.\n",
    "- If the date completed is earlier than the date complained, say NEGATIVE days for item 3.\n",
    "\n",
    "---\n",
    "\n",
    "{DOCUMENT}\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "date_prompt = ChatPromptTemplate.from_template(date_tmpl)\n",
    "date_chain = date_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0cc389",
   "metadata": {},
   "source": [
    "Again, starting with a sample set rather than the full table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95ae9caa",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "less_dates = dates.sample(20).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "675d0b94",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "less_dates['q_dateinfo'] = less_dates.complaint_meta.apply(query_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d11863a5",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# add a method to help fish the answer out of the response string\n",
    "def find_res(v, resn):\n",
    "    pos = v.find(f\"{resn}. \") + 3\n",
    "    if f\"{resn+1}. \" in v:\n",
    "        return v[pos: v.find(f\"{resn+1}. \")].strip()\n",
    "    return v[pos:].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ebb3e19",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# unpack the response column\n",
    "less_dates['q_date_complained'] = less_dates.q_dateinfo.apply(lambda x: find_res(x, 1))\n",
    "less_dates['q_date_completed'] = less_dates.q_dateinfo.apply(lambda x: find_res(x, 2))\n",
    "less_dates['q_time_to_complete'] = less_dates.q_dateinfo.apply(lambda x: find_res(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82ea51f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert (less_dates.date_complained.astype(str) != less_dates.q_date_complained).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0138d6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert (less_dates.date_completed.astype(str) != less_dates.q_date_completed).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b8650cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_complained</th>\n",
       "      <th>q_date_complained</th>\n",
       "      <th>date_completed</th>\n",
       "      <th>q_date_completed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date_complained, q_date_complained, date_completed, q_date_completed]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_dates.loc[(less_dates.date_complained.astype(str) != less_dates.q_date_complained) |\n",
    "               (less_dates.date_completed.astype(str) != less_dates.q_date_completed),\n",
    "               ['date_complained', 'q_date_complained', 'date_completed', 'q_date_completed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f90ae",
   "metadata": {},
   "source": [
    "It looks extracting dates is a pretty reliable process for most of the texts, enough that we could probably make a test case or assertion out of this statement with a few changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edb00f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_dates.loc[(less_dates.date_complained.astype(str) != less_dates.q_date_complained) |\n",
    "               (less_dates.date_completed.astype(str) != less_dates.q_date_completed),\n",
    "               'complaint_meta'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a6e6f",
   "metadata": {},
   "source": [
    "If the model was wrong in an above response, is there an obvious reason or tweak to be made to the prompt based on what we see in the metadata document? How might we avoid this mistake in the future, or prevent a similar error with other documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ca60d",
   "metadata": {},
   "source": [
    "And finally, how does the challenge question hold up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b17ffd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_to_complete</th>\n",
       "      <th>q_time_to_complete</th>\n",
       "      <th>date_complained</th>\n",
       "      <th>date_completed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16588</th>\n",
       "      <td>65 days</td>\n",
       "      <td>65 days</td>\n",
       "      <td>2009-08-12</td>\n",
       "      <td>2009-10-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>110 days</td>\n",
       "      <td>110 days</td>\n",
       "      <td>2021-10-08</td>\n",
       "      <td>2022-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19167</th>\n",
       "      <td>170 days</td>\n",
       "      <td>170 days</td>\n",
       "      <td>2008-04-22</td>\n",
       "      <td>2008-10-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>55 days</td>\n",
       "      <td>55 days</td>\n",
       "      <td>2022-05-18</td>\n",
       "      <td>2022-07-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15894</th>\n",
       "      <td>69 days</td>\n",
       "      <td>69 days</td>\n",
       "      <td>2009-05-22</td>\n",
       "      <td>2009-07-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3584</th>\n",
       "      <td>75 days</td>\n",
       "      <td>75 days</td>\n",
       "      <td>2021-01-20</td>\n",
       "      <td>2021-04-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20913</th>\n",
       "      <td>62 days</td>\n",
       "      <td>62 days</td>\n",
       "      <td>2007-04-26</td>\n",
       "      <td>2007-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18929</th>\n",
       "      <td>205 days</td>\n",
       "      <td>205 days</td>\n",
       "      <td>2008-03-04</td>\n",
       "      <td>2008-09-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15469</th>\n",
       "      <td>337 days</td>\n",
       "      <td>337 days</td>\n",
       "      <td>2008-06-23</td>\n",
       "      <td>2009-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14945</th>\n",
       "      <td>101 days</td>\n",
       "      <td>101 days</td>\n",
       "      <td>2008-11-18</td>\n",
       "      <td>2009-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19159</th>\n",
       "      <td>164 days</td>\n",
       "      <td>164 days</td>\n",
       "      <td>2008-05-01</td>\n",
       "      <td>2008-10-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20547</th>\n",
       "      <td>269 days</td>\n",
       "      <td>268 days</td>\n",
       "      <td>2006-07-24</td>\n",
       "      <td>2007-04-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719</th>\n",
       "      <td>168 days</td>\n",
       "      <td>168 days</td>\n",
       "      <td>2019-07-22</td>\n",
       "      <td>2020-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>274 days</td>\n",
       "      <td>274 days</td>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>2023-04-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14026</th>\n",
       "      <td>321 days</td>\n",
       "      <td>321 days</td>\n",
       "      <td>2013-12-12</td>\n",
       "      <td>2014-10-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>278 days</td>\n",
       "      <td>278 days</td>\n",
       "      <td>2022-12-09</td>\n",
       "      <td>2023-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5837</th>\n",
       "      <td>310 days</td>\n",
       "      <td>311 days</td>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>2019-06-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17084</th>\n",
       "      <td>308 days</td>\n",
       "      <td>308 days</td>\n",
       "      <td>2009-01-29</td>\n",
       "      <td>2009-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>266 days</td>\n",
       "      <td>266 days</td>\n",
       "      <td>2019-06-18</td>\n",
       "      <td>2020-03-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6709</th>\n",
       "      <td>220 days</td>\n",
       "      <td>220 days</td>\n",
       "      <td>2017-08-06</td>\n",
       "      <td>2018-03-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      time_to_complete q_time_to_complete date_complained date_completed\n",
       "16588          65 days            65 days      2009-08-12     2009-10-16\n",
       "1049          110 days           110 days      2021-10-08     2022-01-26\n",
       "19167         170 days           170 days      2008-04-22     2008-10-09\n",
       "1701           55 days            55 days      2022-05-18     2022-07-12\n",
       "15894          69 days            69 days      2009-05-22     2009-07-30\n",
       "3584           75 days            75 days      2021-01-20     2021-04-05\n",
       "20913          62 days            62 days      2007-04-26     2007-06-27\n",
       "18929         205 days           205 days      2008-03-04     2008-09-25\n",
       "15469         337 days           337 days      2008-06-23     2009-05-26\n",
       "14945         101 days           101 days      2008-11-18     2009-02-27\n",
       "19159         164 days           164 days      2008-05-01     2008-10-12\n",
       "20547         269 days           268 days      2006-07-24     2007-04-19\n",
       "3719          168 days           168 days      2019-07-22     2020-01-06\n",
       "383           274 days           274 days      2022-07-21     2023-04-21\n",
       "14026         321 days           321 days      2013-12-12     2014-10-29\n",
       "901           278 days           278 days      2022-12-09     2023-09-13\n",
       "5837          310 days           311 days      2018-07-30     2019-06-05\n",
       "17084         308 days           308 days      2009-01-29     2009-12-03\n",
       "4007          266 days           266 days      2019-06-18     2020-03-10\n",
       "6709          220 days           220 days      2017-08-06     2018-03-14"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_dates[['time_to_complete', 'q_time_to_complete', 'date_complained', 'date_completed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b11ffa",
   "metadata": {},
   "source": [
    "### models vs. libraries\n",
    "\n",
    "In this dataset, `time_to_complete` was calculated by the `pandas` and `datetime` packages, which I expect to be more accurate than the model's math. \n",
    "\n",
    "That being said, I tested this code with two different [available models](https://platform.openai.com/docs/models/gpt-3-5) and they performed quite differently when it came to this challenge Q/A.\n",
    "\n",
    "The first test used a legacy LLM, `text-davinci-003`, and while some of the numbers were pretty close, especially when the true value was under 60 days, longer time periods were significantly off in the time calculation. This is likely do to a naive model's assumption that every month has 30 days, or something similar. However, a more recent model like the one used in this demo, `gpt-3.5-turbo`, appears to do signficantly better at making accurate calculations with time data. \n",
    "\n",
    "A programming library could be expected to check the true possible number of days when doing a calculation between two `datetime` values, and an LLM might be correct if prompted about the number of days in a particular month, but it might not be realistic to assume that any LLM will do this kind of calculation correctly. \n",
    "\n",
    "**TLDR;** Unless you've already confirmed the selected model will make the selected calculations appropriately for your purpose, it's worth checking the documentation and running some examples first. Eventually, it might be the case that all production models have this knowledge, but it's good to be cautious.\n",
    "\n",
    "Regardless of how the time difference math went, the date extraction appears pretty solid, so if we wanted to we could get the dates from the documents that way and then calculate the time to complete the investigation using known libraries, which would still be super helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab54f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
